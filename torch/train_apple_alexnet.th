require 'image'
npy4th = require 'npy4th'
require 'apple_data';
require 'cunn'
require 'cudnn'
require 'nn';
require 'optim'
npy4th=require 'npy4th';
require 'torchx';
require 'gnuplot';
dump=require 'dump';
visualize=require 'visualize';

function getOptimStateTotal(params,parameters,layer_pivot,logger)
    local optimStateTotal={}

    for layer_num=1,#parameters do
        local str=''..layer_num;
        for layer_size_idx=1,#parameters[layer_num]:size() do
            str=str..' '..parameters[layer_num]:size(layer_size_idx);
        end

        local learningRate_curr=params.learningRate;
        if layer_num<=layer_pivot[1] then
            learningRate_curr=1;
        elseif layer_num>=layer_pivot[2] then
            learningRate_curr=learningRate_curr;
        else
            learningRate_curr=1/100*learningRate_curr;
        end

        local optimState_curr={learningRate=learningRate_curr,
                learningRateDecay=params.learningRateDecay ,
                beta1=params.beta1 ,
                beta2=params.beta2 ,
                epsilon=params.epsilon };

        str=str..' '..optimState_curr.learningRate;
        print (str);
        logger:writeString(dump.tostring(str)..'\n');
        optimStateTotal[#optimStateTotal+1]=optimState_curr;
    end
    return optimStateTotal;
end

function doTheUpdate(td,net,criterion,parameters,gradParameters,optimMethod,optimStateTotal)
    
    td:getTrainingData();
    
    if td.training_set.data:type()~='torch.CudaTensor' then
        td.training_set.data = td.training_set.data:cuda();
        td.training_set.label = td.training_set.label:cuda();
    end    
    local batch_inputs = td.training_set.data;
    local batch_targets = td.training_set.label;
    
    net:zeroGradParameters();
    local loss = criterion:forward(net:forward(batch_inputs),batch_targets);
    net:backward(batch_inputs, criterion:backward(net.output, batch_targets))
    
    for layer_num=1, #parameters do
        -- idx_layer_num,layer_num  in pairs({1,2,#parameters-1,#parameters}) do
        -- print (layer_num);
        local fevalScoreVar = function(x)
            return loss, gradParameters[layer_num]
        end
        optimMethod(fevalScoreVar, parameters[layer_num], optimStateTotal[layer_num]);
    end

    return loss;
end

-- local fevalScore = function(x)
--     if x ~= parameters then
-- 	    parameters:copy(x)
--     end
--     -- local t=os.clock();
--     td:getTrainingData();
--     -- print ('done training data',os.clock()-t);
    
--     -- local batch_inputs = td.training_set.data:cuda();
--     -- local batch_targets = td.training_set.label:cuda();

--     -- t=os.clock();
--     if td.training_set.data:type()~='torch.CudaTensor' then
--         td.training_set.data = td.training_set.data:cuda();
--         td.training_set.label = td.training_set.label:cuda();
--     end    
--     local batch_inputs = td.training_set.data;
--     local batch_targets = td.training_set.label;
--     -- print ('moving cuda',os.clock()-t);
    
--     -- local batch_inputs = td.training_set.data:clone();
--     -- local batch_targets = td.training_set.label:clone();
    
--     -- t=os.clock();
--     net:zeroGradParameters();
--     local loss = criterion:forward(net:forward(batch_inputs),batch_targets);
--     net:backward(batch_inputs, criterion:backward(net.output, batch_targets))
--     -- print ('forward backward',os.clock()-t);

--     return loss, gradParameters;
-- end


function main(params) 
    print ('setting_threads');
    torch.setnumthreads(1);
    if params.limit<0 then
        params.limit=nil;
    end
    
	local out_dir=params.outDir
    
    
    paths.mkdir(out_dir);
    local out_dir_intermediate=paths.concat(out_dir,'intermediate');
    local out_dir_final=paths.concat(out_dir,'final');
    paths.mkdir(out_dir_intermediate);
    paths.mkdir(out_dir_final);
    
    local out_file_net=out_dir_final..'/'..'model_all_final.dat';

    local out_file_loss=out_dir_final..'/'..'loss_final.npy';
    
    local out_file_intermediate_pre = out_dir_intermediate..'/'..'model_all_';
    local out_file_loss_intermediate_pre = out_dir_intermediate..'/'..'loss_all_';
    
    local out_file_intermediate_pre = paths.concat(out_dir_intermediate,'model_all_');
    local out_file_loss_intermediate_pre = paths.concat(out_dir_intermediate,'loss_all_');

    local out_file_loss_plot=paths.concat(out_dir_intermediate,'loss_all.png');
    local out_file_log=paths.concat(out_dir_intermediate,'log.txt');
    local logger=torch.DiskFile(out_file_log,'w');

    logger:writeString(dump.tostring(params)..'\n');
    print (params);

    cutorch.setDevice(params.gpu);

    
	-- optimState={learningRate=params.learningRate,
 --        learningRateDecay=params.learningRateDecay ,
 --        beta1=params.beta1 ,
 --        beta2=params.beta2 ,
 --        epsilon=params.epsilon }


    print ('loading network');
    logger:writeString(dump.tostring('loading network')..'\n');
    net = torch.load(params.model);
    
    print ('done loading network');
    logger:writeString(dump.tostring('done loading network')..'\n');
    print (net);


    logger:writeString(dump.tostring('making cuda')..'\n');
    print ('making cuda');
    local net = net:cuda();
    logger:writeString(dump.tostring('done')..'\n');
    print ('done');

    print ('loading params');
    logger:writeString(dump.tostring('loading params')..'\n');
    local parameters, gradParameters = net:parameters()
    logger:writeString(dump.tostring('loading done')..'\n');
    print ('loading done');
    
    local optimState       
    local optimMethod      

    optimMethod = optim.adam
    local layer_pivot={2,#parameters-1};
    local optimStateTotal=getOptimStateTotal(params,parameters,layer_pivot,logger)    

    
    local td_params={file_path_pos = params.pos_path,
                    file_path_neg = params.neg_path,
                    mean_file = params.mean_path,
                    shuffle = params.shuffle,
                    batch_size_pos = params.batch_size_pos,
                    batch_size_neg = params.batch_size_neg,
                    limit = params.limit};
    local td = data(td_params)

    local vd;
    if params.testAfter>0 then
        local vd_params={file_path_pos = params.val_pos_path,
                    file_path_neg = params.val_neg_path,
                    mean_file = params.mean_path,
                    shuffle = false,
                    batch_size_pos = params.batch_size_pos,
                    batch_size_neg = params.batch_size_neg,
                    limit = params.limit};

        vd = data(vd_params)    
    end
    

    
    local criterion = nn.MarginCriterion():cuda();
    -- SoftMarginCriterion():cuda();
    -- 
    -- print (criterion.sizeAverage);
    -- criterion.sizeAverage=false;
 
    local losses = {};
    local losses_iter={};
    local val_losses={};
    local val_losses_iter={};

    for i=1,params.iterations do
    	
        -- local _, minibatch_loss = optimMethod(fevalScore,parameters, optimState)
        local minibatch_loss = doTheUpdate(td,net,criterion,parameters,gradParameters,optimMethod,optimStateTotal)
        losses[#losses + 1] = minibatch_loss -- append the new loss        
        losses_iter[#losses_iter+1]=i;

        if i%params.dispAfter==0 then
            local disp_str=string.format("lr: %6s, minibatches processed: %6s, loss = %6.6f", optimStateTotal[1].learningRate,i, 
                losses[#losses])
            print(disp_str);
            logger:writeString(dump.tostring(disp_str)..'\n');
            
            local str_score=''..losses[#losses];
            
            if str_seg=='nan' or str_score=='nan' then
                print('QUITTING');
                logger:writeString(dump.tostring('QUITTING')..'\n');
                break;
            end
        end

        if i%params.testAfter==0 and params.testAfter>0 then 
            net:evaluate();
            vd:getTrainingData();
            if vd.training_set.data:type()~='torch.CudaTensor' then
                vd.training_set.data = vd.training_set.data:cuda();
                vd.training_set.label = vd.training_set.label:cuda();
            end    
            local batch_inputs = vd.training_set.data;
            local batch_targets = vd.training_set.label;
            local loss = criterion:forward(net:forward(batch_inputs),batch_targets);
            
            val_losses[#val_losses+1]=loss;
            val_losses_iter[#val_losses_iter+1]=i;

            net:training();
            
            disp_str=string.format("minibatches processed: %6s, val loss = %6.6f", i, val_losses[#val_losses])
            logger:writeString(dump.tostring(disp_str)..'\n');
            print(disp_str)
        end

        if i%params.dispPlotAfter==0 then
            visualize:plotLossFigure(losses,losses_iter,val_losses,val_losses_iter,out_file_loss_plot);
        end

        -- check if model needs to be saved. save it.
        -- also save losses
        if i%params.saveAfter==0 then
            local out_file_intermediate=out_file_intermediate_pre..i..'.dat';
            net:clearState();
            torch.save(out_file_intermediate,net);
            local out_file_loss_intermediate=out_file_loss_intermediate_pre..i..'.npy';
            npy4th.savenpy(out_file_loss_intermediate, torch.Tensor(losses))        
        end
	end

    -- save final model
    net:clearState();
    torch.save(out_file_net,net);
    npy4th.savenpy(out_file_loss, torch.Tensor(losses))
    
end



epoch_size=20
cmd = torch.CmdLine()
cmd:text()
cmd:text('Train Apple Detection network')
cmd:text()
cmd:text('Options')
cmd:option('-model','../models/alex_conv_dropout.dat','model to load')
cmd:option('-outDir','../experiments/train_check','directory to write output');

cmd:option('-limit',-1,'num of training data to read');
cmd:option('-shuffle',true,'shuffle input data');
cmd:option('-batch_size_neg',6,'batch size negative data'); 
cmd:option('-batch_size_pos',6,'batch size positive data'); 

cmd:option('-iterations',50*epoch_size,'num of iterations to run');
cmd:option('-saveAfter',5*epoch_size,'num of iterations after which to save model');
cmd:option('-dispAfter',1,'num iterations after which to display training loss');
cmd:option('-testAfter',1*epoch_size,'num iterations after which to display training loss');
cmd:option('-dispPlotAfter',1*epoch_size,'num iterations after which to plot loss');

cmd:option('-pos_path','../data/pos_train_small.txt')
cmd:option('-neg_path','../data/neg_train_small.txt')
cmd:option('-mean_path','../data/mean_im_small.npy')

cmd:option('-val_pos_path','../data/pos_test_small.txt')
cmd:option('-val_neg_path','../data/neg_test_small.txt')

cmd:option('learningRate', 1e-2)
cmd:option('learningRateDecay',5e-6)
cmd:option('beta1', 0.9)
cmd:option('beta2', 0.999)
cmd:option('epsilon', 1e-8)

cmd:option('-gpu',1,'gpu to run the training on');
cmd:text()

params = cmd:parse(arg)
main(params);

